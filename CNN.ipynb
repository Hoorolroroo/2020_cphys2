{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 패션 이미지를 CNN을 이용하여 분류하세요. CNN을 이용한 분류 결과를 MLP와 비교해보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import models \n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import fashion_mnist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "\n",
    "# preprocessing\n",
    "X_train = X_train.reshape((60000, 28*28))\n",
    "X_train = X_train/255\n",
    "\n",
    "X_test = X_test.reshape((10000, 28*28))\n",
    "X_test = X_test/255\n",
    "\n",
    "net = models.Sequential()\n",
    "net.add(layers.Dense(512, activation='relu', input_shape=(28*28,))) \n",
    "net.add(layers.Dense(512, activation='relu')) \n",
    "net.add(layers.Dropout(0.5))\n",
    "net.add(layers.Dense(512, activation='relu')) \n",
    "net.add(layers.Dropout(0.5))\n",
    "net.add(layers.Dense(10, activation='softmax')) \n",
    "net.compile(optimizer='nadam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:10000]\n",
    "partial_X_train = X_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 0.5751 - acc: 0.7940 - val_loss: 0.7241 - val_acc: 0.7552\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.4218 - acc: 0.8508 - val_loss: 0.3761 - val_acc: 0.8656\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3835 - acc: 0.8655 - val_loss: 0.4576 - val_acc: 0.8304\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.3638 - acc: 0.8722 - val_loss: 0.3856 - val_acc: 0.8586\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3448 - acc: 0.8773 - val_loss: 0.3435 - val_acc: 0.8749\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.3401 - acc: 0.8794 - val_loss: 0.3744 - val_acc: 0.8577\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.3339 - acc: 0.8804 - val_loss: 0.3432 - val_acc: 0.8794\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.3219 - acc: 0.8847 - val_loss: 0.3782 - val_acc: 0.8587\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 0.3133 - acc: 0.8869 - val_loss: 0.3497 - val_acc: 0.8763\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 0.3177 - acc: 0.8869 - val_loss: 0.3723 - val_acc: 0.8740\n"
     ]
    }
   ],
   "source": [
    "val= net.fit(partial_X_train,partial_y_train,epochs=10, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 244us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = net.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651000261306763"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Q_train, w_train), (Q_test, w_test) = fashion_mnist.load_data()\n",
    "\n",
    "# preprocessing\n",
    "Q_train = Q_train.reshape((60000, 28, 28, 1))\n",
    "Q_train = Q_train.astype('float32') / 255\n",
    "\n",
    "Q_test = Q_test.reshape((10000, 28, 28, 1))\n",
    "Q_test = Q_test.astype('float32') / 255\n",
    "\n",
    "w_train=to_categorical(w_train)\n",
    "w_test=to_categorical(w_test)\n",
    "\n",
    "Q_val = Q_train[:10000]\n",
    "partial_Q_train = Q_train[10000:]\n",
    "\n",
    "w_val = w_train[:10000]\n",
    "partial_w_train = w_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nel = models.Sequential()\n",
    "nel.add(layers.Conv2D(32,(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "nel.add(layers.MaxPooling2D((2,2)))\n",
    "nel.add(layers.Conv2D(64,(3,3), activation='relu')) \n",
    "nel.add(layers.MaxPooling2D((2,2)))\n",
    "nel.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
    "nel.add(layers.Flatten())\n",
    "nel.add(layers.Dense(512, activation='relu')) \n",
    "nel.add(layers.Dropout(0.5)) \n",
    "nel.add(layers.Dense(64, activation='relu')) \n",
    "nel.add(layers.Dropout(0.5))\n",
    "nel.add(layers.Dense(10, activation='softmax')) \n",
    "nel.compile(optimizer='nadam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 92s 2ms/step - loss: 0.6367 - acc: 0.7693 - val_loss: 0.3950 - val_acc: 0.8476\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4006 - acc: 0.8602 - val_loss: 0.3267 - val_acc: 0.8849\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 92s 2ms/step - loss: 0.3382 - acc: 0.8814 - val_loss: 0.3264 - val_acc: 0.8730\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 103s 2ms/step - loss: 0.3064 - acc: 0.8933 - val_loss: 0.2647 - val_acc: 0.9044\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 0.2808 - acc: 0.8997 - val_loss: 0.2889 - val_acc: 0.8950\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 0.2699 - acc: 0.9055 - val_loss: 0.2755 - val_acc: 0.8986\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 102s 2ms/step - loss: 0.2503 - acc: 0.9099 - val_loss: 0.2883 - val_acc: 0.9024\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 0.2419 - acc: 0.9136 - val_loss: 0.2607 - val_acc: 0.9086\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 0.2255 - acc: 0.9197 - val_loss: 0.2737 - val_acc: 0.9090\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 91s 2ms/step - loss: 0.2166 - acc: 0.9226 - val_loss: 0.2619 - val_acc: 0.9132\n"
     ]
    }
   ],
   "source": [
    "val= nel.fit(partial_Q_train,partial_w_train,epochs=10, batch_size=64, validation_data=(Q_val, w_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 650us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = nel.evaluate(Q_test,w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9017000198364258"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>86.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>90.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy\n",
       "MLP    86.5%\n",
       "CNN    90.1%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=np.array([['86.5%'], ['90.1%']]), index= ['MLP', 'CNN'], columns=['Accuracy'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP보다 CNN을 이용한 기법이 정확도가 좀 더 높게 나왔습니다.\n",
    "아무래도 합성곱을 이용한 이미지 분석이 정확도를 높이는데 많은 기여를 한 것 같습니다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
